{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7DCzf-EHzL_"
   },
   "source": [
    "# CLIPDraw wave\n",
    "Synthesize drawings to match a sound/music prompt\n",
    "\n",
    "Code used from:\n",
    "\n",
    "clipdraw: https://github.com/kvfrans/clipdraw\n",
    "\n",
    "diffvg: https://github.com/BachiLi/diffvg/blob/master/apps/painterly_rendering.py\n",
    "\n",
    "wav2clip: https://github.com/descriptinc/lyrebird-wav2clip\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "5dyyH781qzIC"
   },
   "outputs": [],
   "source": [
    "#@title Pre Installation {vertical-output: true}\n",
    "\n",
    "import subprocess\n",
    "\n",
    "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
    "print(\"CUDA version:\", CUDA_version)\n",
    "\n",
    "%cd /content/\n",
    "!mkdir res\n",
    "!pip install svgwrite\n",
    "!pip install svgpathtools\n",
    "!pip install cssutils\n",
    "!pip install numba\n",
    "!pip install torch-tools\n",
    "!pip install visdom\n",
    "!pip install wav2clip\n",
    "!pip install librosa\n",
    "\n",
    "!git clone https://github.com/BachiLi/diffvg\n",
    "%cd diffvg\n",
    "# !ls\n",
    "!git submodule update --init --recursive\n",
    "!python setup.py install\n",
    "\n",
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hjt9T3ARukAg",
    "outputId": "add047b2-7848-40d8-f7de-589706feab3e"
   },
   "outputs": [],
   "source": [
    "#@title Imports and Notebook Utilities {vertical-output: true}\n",
    "\n",
    "import os\n",
    "import io\n",
    "import PIL.Image, PIL.ImageDraw\n",
    "import base64\n",
    "import zipfile\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pylab as pl\n",
    "import glob\n",
    "\n",
    "from IPython.display import Image, HTML, clear_output\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "\n",
    "os.environ['FFMPEG_BINARY'] = 'ffmpeg'\n",
    "import moviepy.editor as mvp\n",
    "from moviepy.video.io.ffmpeg_writer import FFMPEG_VideoWriter\n",
    "\n",
    "# %cd /content/diffvg/apps/\n",
    "import pydiffvg\n",
    "import skimage\n",
    "import skimage.io\n",
    "import random\n",
    "import ttools.modules\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "import clip\n",
    "import wav2clip\n",
    "import librosa\n",
    "import youtube_dl\n",
    "\n",
    "def imread(url, max_size=None, mode=None):\n",
    "    if url.startswith(('http:', 'https:')):\n",
    "        r = requests.get(url)\n",
    "        f = io.BytesIO(r.content)\n",
    "    else:\n",
    "        f = url\n",
    "        img = PIL.Image.open(f)\n",
    "        \n",
    "    if max_size is not None:\n",
    "        img = img.resize((max_size, max_size))\n",
    "    if mode is not None:\n",
    "        img = img.convert(mode)\n",
    "        \n",
    "    img = np.float32(img)/255.0\n",
    "    return img\n",
    "\n",
    "def np2pil(a):\n",
    "    if a.dtype in [np.float32, np.float64]:\n",
    "        a = np.uint8(np.clip(a, 0, 1)*255)\n",
    "    return PIL.Image.fromarray(a)\n",
    "\n",
    "def imwrite(f, a, fmt=None):\n",
    "    a = np.asarray(a)\n",
    "    if isinstance(f, str):\n",
    "        fmt = f.rsplit('.', 1)[-1].lower()\n",
    "        if fmt == 'jpg':\n",
    "            fmt = 'jpeg'\n",
    "        f = open(f, 'wb')\n",
    "    np2pil(a).save(f, fmt, quality=95)\n",
    "\n",
    "def imencode(a, fmt='jpeg'):\n",
    "    a = np.asarray(a)\n",
    "    if len(a.shape) == 3 and a.shape[-1] == 4:\n",
    "        fmt = 'png'\n",
    "    f = io.BytesIO()\n",
    "    imwrite(f, a, fmt)\n",
    "    return f.getvalue()\n",
    "\n",
    "def im2url(a, fmt='jpeg'):\n",
    "    encoded = imencode(a, fmt)\n",
    "    base64_byte_string = base64.b64encode(encoded).decode('ascii')\n",
    "    return 'data:image/' + fmt.upper() + ';base64,' + base64_byte_string\n",
    "\n",
    "def imshow(a, fmt='jpeg'):\n",
    "    display(Image(data=imencode(a, fmt)))\n",
    "\n",
    "\n",
    "def tile2d(a, w=None):\n",
    "    a = np.asarray(a)\n",
    "    if w is None:\n",
    "        w = int(np.ceil(np.sqrt(len(a))))\n",
    "    th, tw = a.shape[1:3]\n",
    "    pad = (w-len(a))%w\n",
    "    a = np.pad(a, [(0, pad)]+[(0, 0)]*(a.ndim-1), 'constant')\n",
    "    h = len(a)//w\n",
    "    a = a.reshape([h, w]+list(a.shape[1:]))\n",
    "    a = np.rollaxis(a, 2, 1).reshape([th*h, tw*w]+list(a.shape[4:]))\n",
    "    return a\n",
    "\n",
    "from torchvision import utils\n",
    "def show_img(img):\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    img = np.clip(img, 0, 1)\n",
    "    img = np.uint8(img * 254)\n",
    "    # img = np.repeat(img, 4, axis=0)\n",
    "    # img = np.repeat(img, 4, axis=1)\n",
    "    pimg = PIL.Image.fromarray(img, mode=\"RGB\")\n",
    "    imshow(pimg)\n",
    "\n",
    "def zoom(img, scale=4):\n",
    "    img = np.repeat(img, scale, 0)\n",
    "    img = np.repeat(img, scale, 1)\n",
    "    return img\n",
    "\n",
    "class VideoWriter:\n",
    "    def __init__(self, filename='_autoplay.mp4', fps=30.0, **kw):\n",
    "        self.writer = None\n",
    "        self.params = dict(filename=filename, fps=fps, **kw)\n",
    "\n",
    "    def add(self, img):\n",
    "        img = np.asarray(img)\n",
    "        if self.writer is None:\n",
    "            h, w = img.shape[:2]\n",
    "            self.writer = FFMPEG_VideoWriter(size=(w, h), **self.params)\n",
    "        if img.dtype in [np.float32, np.float64]:\n",
    "            img = np.uint8(img.clip(0, 1)*255)\n",
    "        if len(img.shape) == 2:\n",
    "            img = np.repeat(img[..., None], 3, -1)\n",
    "        self.writer.write_frame(img)\n",
    "\n",
    "    def close(self):\n",
    "        if self.writer:\n",
    "            self.writer.close()\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *kw):\n",
    "        self.close()\n",
    "        if self.params['filename'] == '_autoplay.mp4':\n",
    "            self.show()\n",
    "\n",
    "    def show(self, **kw):\n",
    "        self.close()\n",
    "        fn = self.params['filename']\n",
    "        display(mvp.ipython_display(fn, **kw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download music\n",
    "path = 'https://www.youtube.com/watch?v=XdlIbNrki5o',\n",
    "outname = './music.mp3'\n",
    "print(outname)\n",
    "ydl_opts = {\n",
    "        'cachedir': False,\n",
    "        'outtmpl': outname,\n",
    "        'format': 'bestaudio/best',\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'mp3',\n",
    "            'preferredquality': '192',\n",
    "        }],\n",
    "}\n",
    "with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Settings {vertical-output: true}\n",
    "# ARGUMENTS. Feel free to play around with these, especially num_paths.\n",
    "args = lambda: None\n",
    "args.num_paths = 256 #@param {type:\"number\"}\n",
    "args.num_iter = 2000 #@param {type:\"number\"}\n",
    "args.max_width = 50 #@param {type:\"number\"}\n",
    "args.audio_prompt = './music.mp3' #@param {type:\"string\"}\n",
    "args.audio_sampling_freq = 16000 #@param {type:\"number\"}\n",
    "args.gamma = 1.0  #@param {type:\"number\"}\n",
    "args.canvas_size = 224 #@param {type:\"number\"}\n",
    "args.num_augs = 4 #@param {type:\"number\"}\n",
    "\n",
    "# In the CLIPDraw code used to generate examples, they don't normalize images\n",
    "# before passing into CLIP, but really you should. Turn this to True to do that.\n",
    "args.use_normalized_clip = False #@param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z-Wt7UjTi8Le",
    "outputId": "73f1514b-5e05-4275-a54a-789ba6213998"
   },
   "outputs": [],
   "source": [
    "#@title Load Wav2CLIP {vertical-output: true}\n",
    "# Load WaveCLIP model\n",
    "wav2clip_model = wav2clip.get_model().to(device)\n",
    "\n",
    "# Calculate sound features\n",
    "audio, sr = librosa.load(args.audio_prompt, sr=args.audio_sampling_freq)\n",
    "music_features = wav2clip.embed_audio(torch.from_numpy(audio), wav2clip_model)\n",
    "music_features = music_features.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XIVMSJuWgxG"
   },
   "outputs": [],
   "source": [
    "#@title Run {vertical-output: true}\n",
    "\n",
    "pydiffvg.set_print_timing(False)\n",
    "\n",
    "# Use GPU if available\n",
    "pydiffvg.set_use_gpu(torch.cuda.is_available())\n",
    "device = torch.device('cuda')\n",
    "pydiffvg.set_device(device)\n",
    "\n",
    "canvas_width, canvas_height = args.canvas_size, args.canvas_size\n",
    "num_paths = args.num_paths\n",
    "max_width = args.max_width\n",
    "\n",
    "# Image Augmentation Transformation\n",
    "augment_trans = transforms.Compose([\n",
    "    transforms.RandomPerspective(fill=1, p=1, distortion_scale=0.5),\n",
    "    transforms.RandomResizedCrop(args.canvas_size, scale=(0.7,0.9)),\n",
    "])\n",
    "\n",
    "if args.use_normalized_clip:\n",
    "    augment_trans = transforms.Compose([\n",
    "    transforms.RandomPerspective(fill=1, p=1, distortion_scale=0.5),\n",
    "    transforms.RandomResizedCrop(args.canvas_size, scale=(0.7,0.9)),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "\n",
    "# Initialize Random Curves\n",
    "shapes = []\n",
    "shape_groups = []\n",
    "for i in range(num_paths):\n",
    "    num_segments = random.randint(1, 3)\n",
    "    num_control_points = torch.zeros(num_segments, dtype = torch.int32) + 2\n",
    "    points = []\n",
    "    p0 = (random.random(), random.random())\n",
    "    points.append(p0)\n",
    "    for j in range(num_segments):\n",
    "        radius = 0.1\n",
    "        p1 = (p0[0] + radius * (random.random() - 0.5), p0[1] + radius * (random.random() - 0.5))\n",
    "        p2 = (p1[0] + radius * (random.random() - 0.5), p1[1] + radius * (random.random() - 0.5))\n",
    "        p3 = (p2[0] + radius * (random.random() - 0.5), p2[1] + radius * (random.random() - 0.5))\n",
    "        points.append(p1)\n",
    "        points.append(p2)\n",
    "        points.append(p3)\n",
    "        p0 = p3\n",
    "    points = torch.tensor(points)\n",
    "    points[:, 0] *= canvas_width\n",
    "    points[:, 1] *= canvas_height\n",
    "    path = pydiffvg.Path(num_control_points = num_control_points, points = points, stroke_width = torch.tensor(1.0), is_closed = False)\n",
    "    shapes.append(path)\n",
    "    path_group = pydiffvg.ShapeGroup(shape_ids = torch.tensor([len(shapes) - 1]), fill_color = None, stroke_color = torch.tensor([random.random(), random.random(), random.random(), random.random()]))\n",
    "    shape_groups.append(path_group)\n",
    "\n",
    "# Just some diffvg setup\n",
    "scene_args = pydiffvg.RenderFunction.serialize_scene(canvas_width, canvas_height, shapes, shape_groups)\n",
    "render = pydiffvg.RenderFunction.apply\n",
    "img = render(canvas_width, canvas_height, 2, 2, 0, None, *scene_args)\n",
    "points_vars = []\n",
    "stroke_width_vars = []\n",
    "color_vars = []\n",
    "for path in shapes:\n",
    "    path.points.requires_grad = True\n",
    "    points_vars.append(path.points)\n",
    "    path.stroke_width.requires_grad = True\n",
    "    stroke_width_vars.append(path.stroke_width)\n",
    "for group in shape_groups:\n",
    "    group.stroke_color.requires_grad = True\n",
    "    color_vars.append(group.stroke_color)\n",
    "\n",
    "# Optimizers\n",
    "points_optim = torch.optim.Adam(points_vars, lr=1.0)\n",
    "width_optim = torch.optim.Adam(stroke_width_vars, lr=0.1)\n",
    "color_optim = torch.optim.Adam(color_vars, lr=0.01)\n",
    "\n",
    "# Run the main optimization loop\n",
    "for t in range(args.num_iter):\n",
    "\n",
    "    # Anneal learning rate (makes videos look cleaner)\n",
    "    if t == int(args.num_iter * 0.5):\n",
    "        for g in points_optim.param_groups:\n",
    "            g['lr'] = 0.4\n",
    "    if t == int(args.num_iter * 0.75):\n",
    "        for g in points_optim.param_groups:\n",
    "            g['lr'] = 0.1\n",
    "    \n",
    "    points_optim.zero_grad()\n",
    "    width_optim.zero_grad()\n",
    "    color_optim.zero_grad()\n",
    "    scene_args = pydiffvg.RenderFunction.serialize_scene(\\\n",
    "        canvas_width, canvas_height, shapes, shape_groups)\n",
    "    img = render(canvas_width, canvas_height, 2, 2, t, None, *scene_args)\n",
    "    img = img[:, :, 3:4] * img[:, :, :3] + torch.ones(img.shape[0], img.shape[1], 3, device = pydiffvg.get_device()) * (1 - img[:, :, 3:4])\n",
    "    if t % 5 == 0:\n",
    "        pydiffvg.imwrite(img.cpu(), './res/iter_{}.png'.format(int(t/5)), gamma=args.gamma)\n",
    "    img = img[:, :, :3]\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.permute(0, 3, 1, 2) # NHWC -> NCHW\n",
    "\n",
    "    loss = 0\n",
    "    NUM_AUGS = args.num_augs\n",
    "    img_augs = []\n",
    "    for n in range(NUM_AUGS):\n",
    "        img_augs.append(augment_trans(img))\n",
    "    im_batch = torch.cat(img_augs)\n",
    "    image_features = model.encode_image(im_batch)\n",
    "    for n in range(NUM_AUGS):\n",
    "        loss -= torch.cosine_similarity(music_features, image_features[n:n+1], dim=1)\n",
    "\n",
    "    # Backpropagate the gradients.\n",
    "    loss.backward()\n",
    "\n",
    "    # Take a gradient descent step.\n",
    "    points_optim.step()\n",
    "    width_optim.step()\n",
    "    color_optim.step()\n",
    "    for path in shapes:\n",
    "        path.stroke_width.data.clamp_(1.0, max_width)\n",
    "    for group in shape_groups:\n",
    "        group.stroke_color.data.clamp_(0.0, 1.0)\n",
    "    \n",
    "    if t % 10 == 0:\n",
    "        show_img(img.detach().cpu().numpy()[0])\n",
    "        # show_img(torch.cat([img.detach(), img_aug.detach()], axis=3).cpu().numpy()[0])\n",
    "        print('render loss:', loss.item())\n",
    "        print('iteration:', t)\n",
    "        with torch.no_grad():\n",
    "            im_norm = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            noun_norm = nouns_features / nouns_features.norm(dim=-1, keepdim=True)\n",
    "            similarity = (100.0 * im_norm @ noun_norm.T).softmax(dim=-1)\n",
    "            values, indices = similarity[0].topk(5)\n",
    "            print(\"\\nTop predictions:\\n\")\n",
    "            for value, index in zip(values, indices):\n",
    "                print(f\"{nouns[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Eru5XUuOCi6c"
   },
   "outputs": [],
   "source": [
    "#@title Video Renderer {vertical-output: true}\n",
    "\n",
    "# Render a picture with each stroke.\n",
    "with torch.no_grad():\n",
    "    for i in range(args.num_paths):\n",
    "        print(i)\n",
    "        scene_args = pydiffvg.RenderFunction.serialize_scene(\\\n",
    "            canvas_width, canvas_height, shapes[:i+1], shape_groups[:i+1])\n",
    "        img = render(canvas_width, canvas_height, 2, 2, t, None, *scene_args)\n",
    "        img = img[:, :, 3:4] * img[:, :, :3] + torch.ones(img.shape[0], img.shape[1], 3, device = pydiffvg.get_device()) * (1 - img[:, :, 3:4])\n",
    "        pydiffvg.imwrite(img.cpu(), './res/stroke_{}.png'.format(i), gamma=gamma)\n",
    "print(\"ffmpeging\")\n",
    "\n",
    "# Convert the intermediate renderings to a video.\n",
    "from subprocess import call\n",
    "call([\"ffmpeg\", \"-y\", \"-framerate\", \"60\", \"-i\",\n",
    "    \"./res/iter_%d.png\", \"-vb\", \"20M\",\n",
    "    \"./res/out.mp4\"])\n",
    "\n",
    "call([\"ffmpeg\", \"-y\", \"-framerate\", \"60\", \"-i\",\n",
    "    \"./res/stroke_%d.png\", \"-vb\", \"20M\",\n",
    "    \"./res/out_strokes.mp4\"])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CLIPDraw AI Telephone.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
